import os
import io
import re
from datetime import datetime
from collections import defaultdict
from typing import Optional
import pandas as pd
from fastapi import FastAPI, UploadFile, File as FastAPIFile, Depends, Request, Query
from fastapi.responses import JSONResponse, HTMLResponse, StreamingResponse
from fastapi.templating import Jinja2Templates
from sqlalchemy import (
    create_engine,
    Column,
    Integer,
    String,
    Float,
    Boolean,
    ForeignKey,
    DateTime,
    Text,
    desc,
)
from sqlalchemy.orm import declarative_base, sessionmaker, relationship, Session

# ========== –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è ==========
app = FastAPI(title="MOS-GSM Duplicate Checker")

# –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —à–∞–±–ª–æ–Ω–æ–≤
os.makedirs("templates", exist_ok=True)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —à–∞–±–ª–æ–Ω–æ–≤
templates = Jinja2Templates(directory="templates")

DATABASE_URL = os.getenv("DATABASE_URL")
if not DATABASE_URL:
    raise RuntimeError("DATABASE_URL is not set")

engine = create_engine(DATABASE_URL)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()

# ========== –ú–æ–¥–µ–ª–∏ –ë–î ==========
class File(Base):
    __tablename__ = "files"
    id = Column(Integer, primary_key=True, index=True)
    filename = Column(String, nullable=False)
    uploaded_at = Column(DateTime, default=datetime.utcnow)
    orders = relationship("OrderRow", back_populates="file")

class OrderRow(Base):
    __tablename__ = "orders"
    id = Column(Integer, primary_key=True, index=True)
    file_id = Column(Integer, ForeignKey("files.id"), nullable=False)
    raw_text = Column(Text)
    order_number = Column(String, index=True)
    order_date = Column(DateTime, nullable=True)
    address = Column(Text)
    payout = Column(Float)
    worker_name = Column(String)
    work_type = Column(String)
    comment = Column(Text)
    parsed_ok = Column(Boolean, default=False)
    is_problematic = Column(Boolean, default=False)
    created_at = Column(DateTime, default=datetime.utcnow)
    file = relationship("File", back_populates="orders")

# ========== –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ë–î ==========
@app.on_event("startup")
def on_startup():
    if os.getenv("RESET_DB") == "true":
        print("‚ö†Ô∏è  RESET_DB=true - –£–¥–∞–ª–µ–Ω–∏–µ –≤—Å–µ—Ö —Ç–∞–±–ª–∏—Ü...")
        Base.metadata.drop_all(bind=engine)
        print("‚úÖ –¢–∞–±–ª–∏—Ü—ã —É–¥–∞–ª–µ–Ω—ã")
    
    Base.metadata.create_all(bind=engine)
    print("‚úÖ –¢–∞–±–ª–∏—Ü—ã —Å–æ–∑–¥–∞–Ω—ã")

def get_db() -> Session:
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

# ========== –ü–∞—Ä—Å–∏–Ω–≥ ==========
ORDER_NUMBER_REGEX = re.compile(r"\b[–ê-–ØA-Z]{2,5}-\d{5,7}\b")

def extract_order_number(text: str) -> str | None:
    if not text:
        return None
    m = ORDER_NUMBER_REGEX.search(text)
    return m.group(0) if m else None

def extract_address(text: str) -> str | None:
    """
    –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –∞–¥—Ä–µ—Å–∞ –∏–∑ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤:
    1. "–æ—Ç –î–î.–ú–ú.–ì–ì–ì–ì –ß–ß:–ú–ú:–°–°, –∞–¥—Ä–µ—Å..."
    2. "–ó–∞–∫–∞–∑ –∫–ª–∏–µ–Ω—Ç–∞ –ö–ê–£–¢-–•–•–•–•–•–• –æ—Ç –î–î.–ú–ú.–ì–ì–ì–ì –ß–ß:–ú–ú:–°–°, –∞–¥—Ä–µ—Å..."
    3. –ü—Ä–æ—Å—Ç–æ –∞–¥—Ä–µ—Å –ø–æ—Å–ª–µ –∑–∞–ø—è—Ç–æ–π
    """
    if not text:
        return None
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω 1: "–æ—Ç [–¥–∞—Ç–∞] [–≤—Ä–µ–º—è], [–∞–¥—Ä–µ—Å]"
    match = re.search(r"–æ—Ç\s+\d{2}\.\d{2}\.\d{4}\s+[\d:]+,\s*(.+)$", text)
    if match:
        address = match.group(1).strip()
        if len(address) > 5:
            return address
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω 2: "–æ—Ç [–¥–∞—Ç–∞] –±–µ–∑ –≤—Ä–µ–º–µ–Ω–∏, [–∞–¥—Ä–µ—Å]"
    match = re.search(r"–æ—Ç\s+\d{2}\.\d{2}\.\d{4}[^,]*,\s*(.+)$", text)
    if match:
        address = match.group(1).strip()
        if len(address) > 5:
            return address
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω 3: –ï—Å–ª–∏ –µ—Å—Ç—å –Ω–æ–º–µ—Ä –∑–∞–∫–∞–∑–∞, –±–µ—Ä—ë–º –≤—Å—ë –ø–æ—Å–ª–µ –∑–∞–ø—è—Ç–æ–π
    if ORDER_NUMBER_REGEX.search(text):
        parts = text.split(',')
        if len(parts) >= 2:
            address = ','.join(parts[1:]).strip()
            if len(address) > 5 and not address.replace(' ', '').replace(':', '').replace('.', '').isdigit():
                return address
    
    return None

def is_template_row(row: dict) -> bool:
    """–§–∏–ª—å—Ç—Ä —à–∞–±–ª–æ–Ω–Ω—ã—Ö —Å—Ç—Ä–æ–∫"""
    joined = " ".join([str(v) for v in row.values() if v is not None]).strip().lower()
    if not joined:
        return True
    
    keywords = ["–∑–∞–∫–∞–∑", "–∫–ª–∏–µ–Ω—Ç", "–º–æ–Ω—Ç–∞–∂", "–¥–∏–∞–≥–Ω–æ—Å—Ç", "–≤—ã–µ–∑–¥", "–∞–¥—Ä–µ—Å", "—Å—É–º–º–∞"]
    if any(k in joined for k in keywords):
        return False
    
    if joined.startswith("–∏—Ç–æ–≥–æ"):
        return True
    
    if not any(ch.isdigit() for ch in joined) and len(joined) < 10:
        return True
    
    return False

def is_worker_header(text: str) -> bool:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç—Ä–æ–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–æ–º –º–æ–Ω—Ç–∞–∂–Ω–∏–∫–∞ (–§–ò–û).
    –ü—Ä–∏–º–µ—Ä—ã: "–í–µ—Ç—Ä–µ–Ω–∫–æ –î–º–∏—Ç—Ä–∏–π", "–í–∏–∫—É–ª–∏–Ω –ê–Ω–¥—Ä–µ–π", "–ì—É–ª—è–µ–≤ –û–ª–µ–≥"
    """
    if not text:
        return False
    
    text = text.strip()
    
    # –ï—Å–ª–∏ –≤ —Å—Ç—Ä–æ–∫–µ –µ—Å—Ç—å –Ω–æ–º–µ—Ä –∑–∞–∫–∞–∑–∞ - —ç—Ç–æ –Ω–µ –∑–∞–≥–æ–ª–æ–≤–æ–∫
    if ORDER_NUMBER_REGEX.search(text):
        return False
    
    # –ï—Å–ª–∏ –µ—Å—Ç—å –¥–∞—Ç–∞ - —ç—Ç–æ –Ω–µ –∑–∞–≥–æ–ª–æ–≤–æ–∫
    if re.search(r'\d{2}\.\d{2}\.\d{4}', text):
        return False
    
    # –£–±–∏—Ä–∞–µ–º –ø–æ—è—Å–Ω–µ–Ω–∏—è –≤ —Å–∫–æ–±–∫–∞—Ö —Ç–∏–ø–∞ "(–æ–ø–ª–∞—Ç–∞ –∫–ª–∏–µ–Ω—Ç–æ–º)"
    text_clean = re.sub(r'\([^)]*\)', '', text).strip()
    
    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞
    words = text_clean.split()
    
    # –ï—Å–ª–∏ 2-3 —Å–ª–æ–≤–∞, –≤—Å–µ –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è —Å –∑–∞–≥–ª–∞–≤–Ω–æ–π –±—É–∫–≤—ã, –∏ –Ω–µ—Ç —Ü–∏—Ñ—Ä
    if 2 <= len(words) <= 3:
        all_capitalized = all(word[0].isupper() for word in words if word)
        has_no_digits = not any(char.isdigit() for char in text_clean)
        has_no_special = not any(char in text_clean for char in ['‚Ññ', '/', '\\'])
        
        if all_capitalized and has_no_digits and has_no_special:
            return True
    
    return False

def normalize_text(text: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
    if not text:
        return ""
    return " ".join(text.lower().strip().split())

# ========== –ê–Ω–∞–ª–∏—Ç–∏–∫–∞ –¥—É–±–ª–µ–π ==========
def row_short(r: OrderRow) -> dict:
    return {
        "id": r.id,
        "file_id": r.file_id,
        "order_number": r.order_number,
        "address": r.address,
        "payout": r.payout,
        "worker_name": r.worker_name,
        "work_type": r.work_type,
        "raw_text": r.raw_text[:100] if r.raw_text else "",
    }

def analyze_duplicates_for_file(db: Session, file_id: int) -> dict:
    """–ê–Ω–∞–ª–∏–∑ —Å —É—á—ë—Ç–æ–º –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Å—Ç—Ä–æ–∫"""
    all_orders: list[OrderRow] = (
        db.query(OrderRow)
        .filter(
            OrderRow.order_number.isnot(None),
            OrderRow.address.isnot(None),
        )
        .all()
    )
    
    clusters = defaultdict(list)
    for r in all_orders:
        key = (
            r.order_number.strip().upper(),
            normalize_text(r.address)
        )
        clusters[key].append(r)
    
    hard_duplicates = []
    combo_clusters = []
    clusters_with_multiple = []
    
    for (order_number, normalized_address), rows in clusters.items():
        if len(rows) < 2:
            continue
        
        original_address = rows[0].address
        clusters_with_multiple.append((order_number, original_address, rows))
        
        by_type = defaultdict(list)
        has_diag_or_insp = False
        has_install = False
        
        for r in rows:
            by_type[r.work_type].append(r)
            if r.work_type in ("diagnostic", "inspection"):
                has_diag_or_insp = True
            if r.work_type == "installation":
                has_install = True
        
        for wt, items in by_type.items():
            if len(items) >= 2:
                hard_duplicates.append({
                    "order_number": order_number,
                    "address": original_address,
                    "work_type": wt,
                    "rows": [row_short(r) for r in items],
                })
        
        if has_diag_or_insp and has_install:
            combo_clusters.append({
                "order_number": order_number,
                "address": original_address,
                "rows": [row_short(r) for r in rows],
            })
    
    # –ü—Ä–æ–±–ª–µ–º–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏
    problematic_orders = (
        db.query(OrderRow)
        .filter(OrderRow.file_id == file_id, OrderRow.is_problematic == True)
        .all()
    )
    
    return {
        "clusters_with_multiple_count": len(clusters_with_multiple),
        "hard_duplicates_count": len(hard_duplicates),
        "combo_clusters_count": len(combo_clusters),
        "problematic_count": len(problematic_orders),
        "hard_duplicates_sample": hard_duplicates[:30],
        "combo_clusters_sample": combo_clusters[:30],
        "problematic_sample": [row_short(r) for r in problematic_orders[:30]],
    }

# ========== API –≠–Ω–¥–ø–æ–∏–Ω—Ç—ã ==========

@app.get("/api/files")
async def api_get_files(db: Session = Depends(get_db)):
    """–°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤"""
    files = db.query(File).order_by(desc(File.uploaded_at)).all()
    
    result = []
    for f in files:
        total_rows = db.query(OrderRow).filter(OrderRow.file_id == f.id).count()
        problematic = db.query(OrderRow).filter(
            OrderRow.file_id == f.id, 
            OrderRow.is_problematic == True
        ).count()
        
        result.append({
            "id": f.id,
            "filename": f.filename,
            "uploaded_at": f.uploaded_at.isoformat(),
            "total_rows": total_rows,
            "problematic_rows": problematic,
        })
    
    return {"files": result}

@app.get("/api/files/{file_id}")
async def api_get_file(file_id: int, db: Session = Depends(get_db)):
    """–î–µ—Ç–∞–ª–∏ —Ñ–∞–π–ª–∞"""
    file = db.query(File).filter(File.id == file_id).first()
    if not file:
        return JSONResponse(status_code=404, content={"error": "File not found"})
    
    analysis = analyze_duplicates_for_file(db, file_id)
    total_rows = db.query(OrderRow).filter(OrderRow.file_id == file_id).count()
    
    return {
        "id": file.id,
        "filename": file.filename,
        "uploaded_at": file.uploaded_at.isoformat(),
        "total_rows": total_rows,
        "analysis": analysis,
    }

@app.get("/api/files/{file_id}/rows")
async def api_get_rows(
    file_id: int,
    page: int = Query(1, ge=1),
    limit: int = Query(50, ge=1, le=500),
    order_number: Optional[str] = None,
    address: Optional[str] = None,
    worker_name: Optional[str] = None,
    work_type: Optional[str] = None,
    problematic_only: bool = False,
    db: Session = Depends(get_db)
):
    """–°—Ç—Ä–æ–∫–∏ —Ñ–∞–π–ª–∞ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –∏ –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π"""
    query = db.query(OrderRow).filter(OrderRow.file_id == file_id)
    
    if order_number:
        query = query.filter(OrderRow.order_number.ilike(f"%{order_number}%"))
    if address:
        query = query.filter(OrderRow.address.ilike(f"%{address}%"))
    if worker_name:
        query = query.filter(OrderRow.worker_name.ilike(f"%{worker_name}%"))
    if work_type:
        query = query.filter(OrderRow.work_type == work_type)
    if problematic_only:
        query = query.filter(OrderRow.is_problematic == True)
    
    total = query.count()
    offset = (page - 1) * limit
    rows = query.offset(offset).limit(limit).all()
    
    return {
        "total": total,
        "page": page,
        "limit": limit,
        "rows": [row_short(r) for r in rows],
    }

@app.delete("/api/files/{file_id}")
async def api_delete_file(file_id: int, db: Session = Depends(get_db)):
    """–£–¥–∞–ª–∏—Ç—å —Ñ–∞–π–ª –∏ –µ–≥–æ –∑–∞–ø–∏—Å–∏"""
    db.query(OrderRow).filter(OrderRow.file_id == file_id).delete()
    db.query(File).filter(File.id == file_id).delete()
    db.commit()
    return {"message": "File deleted successfully"}

@app.post("/api/files/{file_id}/recalc")
async def api_recalc_file(file_id: int, db: Session = Depends(get_db)):
    """–ü–µ—Ä–µ—Å—á–∏—Ç–∞—Ç—å –∞–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–∞"""
    analysis = analyze_duplicates_for_file(db, file_id)
    return {"message": "Analysis recalculated", "analysis": analysis}

@app.get("/api/files/{file_id}/export/{what}")
async def api_export(
    file_id: int,
    what: str,
    db: Session = Depends(get_db)
):
    """–≠–∫—Å–ø–æ—Ä—Ç –≤ CSV"""
    file = db.query(File).filter(File.id == file_id).first()
    if not file:
        return JSONResponse(status_code=404, content={"error": "File not found"})
    
    if what == "rows":
        rows = db.query(OrderRow).filter(OrderRow.file_id == file_id).all()
        data = [row_short(r) for r in rows]
    elif what == "problematic":
        rows = db.query(OrderRow).filter(
            OrderRow.file_id == file_id,
            OrderRow.is_problematic == True
        ).all()
        data = [row_short(r) for r in rows]
    elif what in ["hard", "combo", "clusters"]:
        analysis = analyze_duplicates_for_file(db, file_id)
        if what == "hard":
            data = analysis["hard_duplicates_sample"]
        elif what == "combo":
            data = analysis["combo_clusters_sample"]
        else:
            data = []
    else:
        return JSONResponse(status_code=400, content={"error": "Invalid export type"})
    
    df = pd.DataFrame(data)
    output = io.StringIO()
    df.to_csv(output, index=False, encoding='utf-8-sig')
    output.seek(0)
    
    return StreamingResponse(
        iter([output.getvalue()]),
        media_type="text/csv",
        headers={"Content-Disposition": f"attachment; filename={file.filename}_{what}.csv"}
    )

@app.get("/debug/row/{row_id}")
async def debug_row(row_id: int, db: Session = Depends(get_db)):
    """–ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å –¥–µ—Ç–∞–ª–∏ –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏"""
    row = db.query(OrderRow).filter(OrderRow.id == row_id).first()
    if not row:
        return {"error": "Row not found"}
    
    return {
        "id": row.id,
        "file_id": row.file_id,
        "raw_text": row.raw_text,
        "order_number": row.order_number,
        "address": row.address,
        "payout": row.payout,
        "worker_name": row.worker_name,
        "work_type": row.work_type,
        "comment": row.comment,
        "parsed_ok": row.parsed_ok,
        "is_problematic": row.is_problematic,
    }

@app.post("/upload")
async def upload_file(
    file: UploadFile = FastAPIFile(...),
    db: Session = Depends(get_db),
):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ Excel —Ñ–∞–π–ª–∞"""
    content = await file.read()
    
    try:
        # –ß–∏—Ç–∞–µ–º Excel, –ø—Ä–æ–ø—É—Å–∫–∞—è –ø–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫ (–ø–∞—Ä–∞–º–µ—Ç—Ä—ã)
        df = pd.read_excel(io.BytesIO(content), header=5)
        
        # –û—á–∏—â–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫
        df.columns = [str(col).strip() if col is not None else "" for col in df.columns]
        
        print(f"üîç DEBUG: –í—Å–µ–≥–æ –∫–æ–ª–æ–Ω–æ–∫: {len(df.columns)}")
        print(f"üîç DEBUG: –ü–µ—Ä–≤—ã–µ 5 –∫–æ–ª–æ–Ω–æ–∫: {list(df.columns[:5])}")
        print(f"üîç DEBUG: –ü–æ—Å–ª–µ–¥–Ω–∏–µ 5 –∫–æ–ª–æ–Ω–æ–∫: {list(df.columns[-5:])}")
        
    except Exception as e:
        return JSONResponse(
            status_code=400,
            content={"error": f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å Excel: {str(e)}"},
        )
    
    # –°–æ–∑–¥–∞—ë–º –∑–∞–ø–∏—Å—å –æ —Ñ–∞–π–ª–µ
    db_file = File(filename=file.filename)
    db.add(db_file)
    db.commit()
    db.refresh(db_file)
    
    total_rows = 0
    inserted_rows = 0
    problematic_rows = 0
    
    # ========== –ü–û–ò–°–ö –ö–û–õ–û–ù–û–ö ==========
    
    print(f"\n{'='*60}")
    print(f"üîç –ê–ù–ê–õ–ò–ó –°–¢–†–£–ö–¢–£–†–´ –§–ê–ô–õ–ê")
    print(f"{'='*60}")
    print(f"–í—Å–µ–≥–æ –∫–æ–ª–æ–Ω–æ–∫: {len(df.columns)}")
    
    # –í—ã–≤–æ–¥–∏–º –≤—Å–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
    for idx, col in enumerate(df.columns):
        print(f"  [{idx:2d}] {col}")
    print(f"{'='*60}\n")
    
    # 1. –ö–æ–ª–æ–Ω–∫–∞ –∑–∞–∫–∞–∑–∞ (–ø–µ—Ä–≤–∞—è –∫–æ–ª–æ–Ω–∫–∞)
    order_col = df.columns[0] if len(df.columns) > 0 else None
    print(f"‚úì –ö–æ–ª–æ–Ω–∫–∞ –∑–∞–∫–∞–∑–∞: [{0}] {order_col}")
    
    # 2. –ö–æ–ª–æ–Ω–∫–∞ –º–æ–Ω—Ç–∞–∂–Ω–∏–∫–∞ (—Ç–∞ –∂–µ, —á—Ç–æ –∏ –∑–∞–∫–∞–∑)
    worker_col = df.columns[0] if len(df.columns) > 0 else None
    print(f"‚úì –ö–æ–ª–æ–Ω–∫–∞ –º–æ–Ω—Ç–∞–∂–Ω–∏–∫–∞: [{0}] {worker_col}")
    
    # 3. –ö–û–õ–û–ù–ö–ê "–ò–¢–û–ì–û" - –æ—Å–Ω–æ–≤–Ω–∞—è —Å—É–º–º–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    payout_col = None
    payout_col_idx = None
    
    # –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é (–∏—Å–∫–ª—é—á–∞—è "–í—ã—Ä—É—á–∫–∞ –∏—Ç–æ–≥–æ")
    for idx, c in enumerate(df.columns):
        name = str(c).strip().lower()
        if "–∏—Ç–æ–≥–æ" in name and "–≤—ã—Ä—É—á–∫–∞" not in name:
            payout_col = c
            payout_col_idx = idx
            print(f"‚úì –ö–æ–ª–æ–Ω–∫–∞ '–ò—Ç–æ–≥–æ' –Ω–∞–π–¥–µ–Ω–∞ –ø–æ –∏–º–µ–Ω–∏: [{idx}] {c}")
            break
    
    # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –ø–æ –∏–º–µ–Ω–∏, –∏—â–µ–º –ø–æ –∏–Ω–¥–µ–∫—Å–∞–º (–ø—Ä–æ–±—É–µ–º 16-20)
    if payout_col is None:
        for idx in [18, 17, 19, 16, 20, 15]:
            if idx < len(df.columns):
                col_name = str(df.columns[idx]).strip()
                print(f"  –ü—Ä–æ–≤–µ—Ä—è–µ–º [{idx}]: {col_name}")
                if "–∏—Ç–æ–≥–æ" in col_name.lower() and "–≤—ã—Ä—É—á–∫–∞" not in col_name.lower():
                    payout_col = df.columns[idx]
                    payout_col_idx = idx
                    print(f"‚úì –ö–æ–ª–æ–Ω–∫–∞ '–ò—Ç–æ–≥–æ' –Ω–∞–π–¥–µ–Ω–∞ –ø–æ –∏–Ω–¥–µ–∫—Å—É: [{idx}] {df.columns[idx]}")
                    break
    
    if payout_col is None:
        print("‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –ö–æ–ª–æ–Ω–∫–∞ '–ò—Ç–æ–≥–æ' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
    
    # 4. –ö–û–õ–û–ù–ö–ê "–î–ò–ê–ì–ù–û–°–¢–ò–ö–ê" –∏–ª–∏ "–û–ü–õ–ê–¢–ê –î–ò–ê–ì–ù–û–°–¢–ò–ö–ò"
    diagnostic_col = None
    diagnostic_col_idx = None
    
    # –ò—â–µ–º –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é
    for idx, c in enumerate(df.columns):
        name = str(c).lower()
        if "–¥–∏–∞–≥–Ω–æ—Å—Ç" in name:
            diagnostic_col = c
            diagnostic_col_idx = idx
            print(f"‚úì –ö–æ–ª–æ–Ω–∫–∞ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏: [{idx}] {c}")
            break
    
    # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, –ø—Ä–æ–±—É–µ–º –ø–æ –∏–Ω–¥–µ–∫—Å–∞–º (–æ–±—ã—á–Ω–æ 4 –∏–ª–∏ 5)
    if diagnostic_col is None:
        for idx in [4, 5, 3, 6]:
            if idx < len(df.columns):
                col_name = str(df.columns[idx]).lower()
                if "–¥–∏–∞–≥–Ω–æ—Å—Ç" in col_name:
                    diagnostic_col = df.columns[idx]
                    diagnostic_col_idx = idx
                    print(f"‚úì –ö–æ–ª–æ–Ω–∫–∞ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –Ω–∞–π–¥–µ–Ω–∞ –ø–æ –∏–Ω–¥–µ–∫—Å—É: [{idx}] {df.columns[idx]}")
                    break
    
    if diagnostic_col is None:
        print("‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –ö–æ–ª–æ–Ω–∫–∞ '–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
    
    # 5. –ö–û–õ–û–ù–ö–ê "–í–´–†–£–ß–ö–ê (–í–´–ï–ó–î) –°–ü–ï–¶–ò–ê–õ–ò–°–¢–ê"
    inspection_col = None
    inspection_col_idx = None
    
    # –ò—â–µ–º –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é
    for idx, c in enumerate(df.columns):
        name = str(c).lower()
        # –ò—â–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å "–≤—ã–µ–∑–¥" + "—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç"
        if "–≤—ã–µ–∑–¥" in name and "—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç" in name:
            inspection_col = c
            inspection_col_idx = idx
            print(f"‚úì –ö–æ–ª–æ–Ω–∫–∞ –æ—Å–º–æ—Ç—Ä–∞ (–≤—ã–µ–∑–¥ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞): [{idx}] {c}")
            break
    
    # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, –ø—Ä–æ–±—É–µ–º –ø–æ –∏–Ω–¥–µ–∫—Å–∞–º (–æ–±—ã—á–Ω–æ 6 –∏–ª–∏ 7)
    if inspection_col is None:
        for idx in [6, 7, 5, 8]:
            if idx < len(df.columns):
                col_name = str(df.columns[idx]).lower()
                if "–≤—ã–µ–∑–¥" in col_name and "—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç" in col_name:
                    inspection_col = df.columns[idx]
                    inspection_col_idx = idx
                    print(f"‚úì –ö–æ–ª–æ–Ω–∫–∞ –æ—Å–º–æ—Ç—Ä–∞ –Ω–∞–π–¥–µ–Ω–∞ –ø–æ –∏–Ω–¥–µ–∫—Å—É: [{idx}] {df.columns[idx]}")
                    break
    
    if inspection_col is None:
        print("‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –ö–æ–ª–æ–Ω–∫–∞ '–í—ã—Ä—É—á–∫–∞ (–≤—ã–µ–∑–¥) —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
    
    # 6. –ö–æ–ª–æ–Ω–∫–∞ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤
    comment_col = None
    for idx, c in enumerate(df.columns):
        if "–∫–æ–º–º–µ–Ω—Ç" in str(c).lower():
            comment_col = c
            print(f"‚úì –ö–æ–ª–æ–Ω–∫–∞ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤: [{idx}] {c}")
            break
    
    print(f"\n{'='*60}")
    print(f"–ò–¢–û–ì–û: –ù–∞–π–¥–µ–Ω–æ –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:")
    print(f"  - –ó–∞–∫–∞–∑: {'‚úì' if order_col else '‚úó'}")
    print(f"  - –ò—Ç–æ–≥–æ: {'‚úì' if payout_col else '‚úó'}")
    print(f"  - –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞: {'‚úì' if diagnostic_col else '‚úó'}")
    print(f"  - –û—Å–º–æ—Ç—Ä (–≤—ã–µ–∑–¥): {'‚úì' if inspection_col else '‚úó'}")
    print(f"{'='*60}\n")
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ç—Ä–æ–∫–∏
    for idx, row in df.iterrows():
        total_rows += 1
        row_dict = row.to_dict()
        
        if is_template_row(row_dict):
            continue
        
        text_cell = ""
        if order_col and pd.notna(row.get(order_col)):
            text_cell = str(row.get(order_col)).strip()
        
        if not text_cell:
            text_cell = " ".join([str(v) for v in row_dict.values() if pd.notna(v)])
        
        # –ï—Å–ª–∏ —ç—Ç–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –º–æ–Ω—Ç–∞–∂–Ω–∏–∫–∞ - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
        if is_worker_header(text_cell):
            print(f"‚è≠Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω –∑–∞–≥–æ–ª–æ–≤–æ–∫ –º–æ–Ω—Ç–∞–∂–Ω–∏–∫–∞: {text_cell}")
            continue
        
        order_number = extract_order_number(text_cell)
        address = extract_address(text_cell)
        
        payout_val = None
        if payout_col is not None:
            raw = row.get(payout_col)
            if pd.notna(raw):
                try:
                    if isinstance(raw, str):
                        cleaned = raw.replace(" ", "").replace(",", ".")
                        payout_val = float(cleaned)
                    else:
                        payout_val = float(raw)
                except Exception:
                    payout_val = None
        
        # –°—É–º–º—ã –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∏–ø–∞ —Ä–∞–±–æ—Ç—ã
        diag_sum = 0.0
        if diagnostic_col is not None and pd.notna(row.get(diagnostic_col)):
            try:
                raw_val = row.get(diagnostic_col)
                if isinstance(raw_val, str):
                    val = raw_val.replace(" ", "").replace(",", ".")
                else:
                    val = str(raw_val)
                diag_sum = float(val)
                if diag_sum > 0:
                    print(f"  üí∞ –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞: {diag_sum} ‚ÇΩ (–∑–∞–∫–∞–∑: {order_number})")
            except Exception as e:
                print(f"  ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏: {e}")
                diag_sum = 0.0
        
        insp_sum = 0.0
        if inspection_col is not None and pd.notna(row.get(inspection_col)):
            try:
                raw_val = row.get(inspection_col)
                if isinstance(raw_val, str):
                    val = raw_val.replace(" ", "").replace(",", ".")
                else:
                    val = str(raw_val)
                insp_sum = float(val)
                if insp_sum > 0:
                    print(f"  üëÅÔ∏è  –û—Å–º–æ—Ç—Ä (–≤—ã–µ–∑–¥): {insp_sum} ‚ÇΩ (–∑–∞–∫–∞–∑: {order_number})")
            except Exception as e:
                print(f"  ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ—Å–º–æ—Ç—Ä–∞: {e}")
                insp_sum = 0.0
        
        # DEBUG: –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–∑–≤–ª–µ—á—ë–Ω–Ω—É—é —Å—É–º–º—É –∏–∑ "–ò—Ç–æ–≥–æ"
        if payout_val and payout_val > 0:
            print(f"  üíµ –ò—Ç–æ–≥–æ: {payout_val} ‚ÇΩ (–∑–∞–∫–∞–∑: {order_number})")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Ä–∞–±–æ—Ç—ã (–í–ê–ñ–ù–û: –ø–æ—Ä—è–¥–æ–∫ –∏–º–µ–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ!)
        work_type = "other"
        
        if diag_sum > 0:
            work_type = "diagnostic"
            print(f"  ‚ûú –¢–∏–ø —Ä–∞–±–æ—Ç—ã: –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê")
        elif insp_sum > 0:
            work_type = "inspection"
            print(f"  ‚ûú –¢–∏–ø —Ä–∞–±–æ—Ç—ã: –û–°–ú–û–¢–†")
        elif payout_val is not None and payout_val > 5000:
            work_type = "installation"
            print(f"  ‚ûú –¢–∏–ø —Ä–∞–±–æ—Ç—ã: –ú–û–ù–¢–ê–ñ (–ò—Ç–æ–≥–æ > 5000)")
        else:
            print(f"  ‚ûú –¢–∏–ø —Ä–∞–±–æ—Ç—ã: –î–†–£–ì–û–ï")
        
        worker_name = None
        if worker_col and pd.notna(row.get(worker_col)):
            worker_name = str(row.get(worker_col)).strip()
            if worker_name.lower() in ["–º–æ–Ω—Ç–∞–∂–Ω–∏–∫", "–∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å", "—Ñ–∏–æ", ""]:
                worker_name = None
        
        comment_value = ""
        if comment_col and pd.notna(row.get(comment_col)):
            comment_value = str(row.get(comment_col)).strip()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—Ä–æ–±–ª–µ–º–Ω—É—é —Å—Ç—Ä–æ–∫—É
        is_problematic = False
        parsed_ok = True
        
        if not order_number or not address:
            is_problematic = True
            parsed_ok = False
        
        order_row = OrderRow(
            file_id=db_file.id,
            raw_text=text_cell[:1000] if text_cell else "",
            order_number=order_number,
            address=address,
            payout=payout_val,
            worker_name=worker_name,
            work_type=work_type,
            comment=comment_value,
            parsed_ok=parsed_ok,
            is_problematic=is_problematic,
        )
        db.add(order_row)
        inserted_rows += 1
        if is_problematic:
            problematic_rows += 1
    
    db.commit()
    
    analysis = analyze_duplicates_for_file(db, db_file.id)
    
    return {
        "message": "–§–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω –∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω",
        "file_id": db_file.id,
        "filename": db_file.filename,
        "total_rows_in_file": int(total_rows),
        "saved_rows": int(inserted_rows),
        "problematic_rows": int(problematic_rows),
        "clusters_with_multiple_count": analysis["clusters_with_multiple_count"],
        "hard_duplicates_count": analysis["hard_duplicates_count"],
        "combo_clusters_count": analysis["combo_clusters_count"],
        "problematic_count": analysis["problematic_count"],
        "hard_duplicates_sample": analysis["hard_duplicates_sample"],
        "combo_clusters_sample": analysis["combo_clusters_sample"],
        "problematic_sample": analysis["problematic_sample"],
    }

# ========== UI –≠–Ω–¥–ø–æ–∏–Ω—Ç—ã ==========

@app.get("/", response_class=HTMLResponse)
async def ui_home(request: Request):
    """–ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ - –∑–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞"""
    return templates.TemplateResponse("upload.html", {"request": request})

@app.get("/ui/files", response_class=HTMLResponse)
async def ui_files_list(request: Request, db: Session = Depends(get_db)):
    """–°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤"""
    files = db.query(File).order_by(desc(File.uploaded_at)).all()
    
    files_data = []
    for f in files:
        total_rows = db.query(OrderRow).filter(OrderRow.file_id == f.id).count()
        problematic = db.query(OrderRow).filter(
            OrderRow.file_id == f.id, 
            OrderRow.is_problematic == True
        ).count()
        
        files_data.append({
            "id": f.id,
            "filename": f.filename,
            "uploaded_at": f.uploaded_at.strftime("%d.%m.%Y %H:%M"),
            "total_rows": total_rows,
            "problematic_rows": problematic,
        })
    
    return templates.TemplateResponse("files_list.html", {
        "request": request,
        "files": files_data
    })

@app.get("/ui/files/{file_id}", response_class=HTMLResponse)
async def ui_file_detail(request: Request, file_id: int, db: Session = Depends(get_db)):
    """–î–µ—Ç–∞–ª–∏ —Ñ–∞–π–ª–∞ —Å —Ç–∞–±–∞–º–∏"""
    file = db.query(File).filter(File.id == file_id).first()
    if not file:
        return HTMLResponse(content="<h1>–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω</h1>", status_code=404)
    
    analysis = analyze_duplicates_for_file(db, file_id)
    total_rows = db.query(OrderRow).filter(OrderRow.file_id == file_id).count()
    
    return templates.TemplateResponse("file_detail.html", {
        "request": request,
        "file": file,
        "file_id": file_id,
        "total_rows": total_rows,
        "analysis": analysis,
    })

@app.get("/admin/reset", response_class=HTMLResponse)
async def ui_admin(request: Request):
    """–°—Ç—Ä–∞–Ω–∏—Ü–∞ —Å–µ—Ä–≤–∏—Å–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π"""
    return templates.TemplateResponse("admin.html", {"request": request})

@app.post("/admin/reset/soft")
async def admin_reset_soft(db: Session = Depends(get_db)):
    """–ú—è–≥–∫–∏–π —Å–±—Ä–æ—Å - —É–¥–∞–ª—è–µ—Ç –¥–∞–Ω–Ω—ã–µ"""
    db.query(OrderRow).delete()
    db.query(File).delete()
    db.commit()
    return {"message": "–í—Å–µ –¥–∞–Ω–Ω—ã–µ —É–¥–∞–ª–µ–Ω—ã"}

@app.post("/admin/reset/hard")
async def admin_reset_hard(db: Session = Depends(get_db)):
    """–ñ—ë—Å—Ç–∫–∏–π —Å–±—Ä–æ—Å - —É–¥–∞–ª—è–µ—Ç —Ç–∞–±–ª–∏—Ü—ã"""
    Base.metadata.drop_all(bind=engine)
    Base.metadata.create_all(bind=engine)
    return {"message": "–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∞"}
